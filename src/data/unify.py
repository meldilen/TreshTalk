import os
import csv
import pandas as pd
from pathlib import Path
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split


class DatasetUnifier:
    def __init__(self, raw_dir="data/raw", unified_dir="data/unified"):
        self.raw_dir = Path(raw_dir)
        self.unified_dir = Path(unified_dir)
        self.manifest_path = self.unified_dir / "manifest.csv"

        self.unified_classes = {
            'cardboard', 'paper', 'plastic', 'metal', 'glass', 'trash',
            'battery', 'clothes', 'lamp', 'biological'
        }

        self.class_mapping = self._build_class_mapping()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.dataset_configs = self._build_dataset_configs()

    def _build_class_mapping(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º"""
        mapping = {}

        mapping['trashnet'] = {
            'cardboard': 'cardboard',
            'paper': 'paper',
            'plastic': 'plastic',
            'metal': 'metal',
            'glass': 'glass',
            'trash': 'trash'
        }

        mapping['12classes'] = {
            'cardboard': 'cardboard',
            'paper': 'paper',
            'plastic': 'plastic',
            'metal': 'metal',
            'trash': 'trash',
            'brown-glass': 'glass',
            'green-glass': 'glass',
            'white-glass': 'glass',
            'battery': 'battery',
            'clothes': 'clothes',
            'shoes': 'clothes',
            'biological': 'biological'
        }

        mapping['WaRP'] = {
            # –ü–ª–∞—Å—Ç–∏–∫ - –≤—Å–µ bottle –∏ canister
            'bottle-blue': 'plastic',
            'bottle-blue-full': 'plastic',
            'bottle-blue5l': 'plastic',
            'bottle-blue5l-full': 'plastic',
            'bottle-dark': 'plastic',
            'bottle-dark-full': 'plastic',
            'bottle-green': 'plastic',
            'bottle-green-full': 'plastic',
            'bottle-milk': 'plastic',
            'bottle-milk-full': 'plastic',
            'bottle-multicolor': 'plastic',
            'bottle-multicolor-full': 'plastic',
            'bottle-oil': 'plastic',
            'bottle-oil-full': 'plastic',
            'bottle-transp': 'plastic',
            'bottle-transp-full': 'plastic',
            'bottle-yogurt': 'plastic',
            'canister': 'plastic',

            # –°—Ç–µ–∫–ª–æ
            'glass-dark': 'glass',
            'glass-green': 'glass',
            'glass-transp': 'glass',

            # –ú–µ—Ç–∞–ª–ª
            'cans': 'metal',

            # –ö–∞—Ä—Ç–æ–Ω
            'juice-cardboard': 'cardboard',
            'milk-cardboard': 'cardboard',

            # –ú–æ—é—â–∏–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ (–ø–ª–∞—Å—Ç–∏–∫)
            'detergent': 'plastic',
            'detergent-box': 'plastic',
            'detergent-color': 'plastic',
            'detergent-transparent': 'plastic',
            'detergent-white': 'plastic'
        }

        mapping['garbage_classification_1'] = {
            'cardboard': 'cardboard',
            'glass': 'glass',
            'metal': 'metal',
            'paper': 'paper',
            'plastic': 'plastic',
            'trash': 'trash'
        }

        mapping['trash_type'] = {
            'cardboard': 'cardboard',
            'glass': 'glass',
            'metal': 'metal',
            'paper': 'paper',
            'plastic': 'plastic',
            'trash': 'trash'
        }

        mapping['realwaste'] = {
            'Cardboard': 'cardboard',
            'Food Organics': 'biological',
            'Glass': 'glass',
            'Metal': 'metal',
            'Miscellaneous Trash': 'trash',
            'Paper': 'paper',
            'Plastic': 'plastic',
            'Textile Trash': 'clothes',
            'Vegetation': 'biological'  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∞
        }

        mapping['garbage_classification_2'] = {
            'cardboard': 'cardboard',
            'glass': 'glass',
            'metal': 'metal',
            'paper': 'paper',
            'plastic': 'plastic',
            'trash': 'trash'
        }

        mapping['garbage_dataset'] = {
            'battery': 'battery',
            'biological': 'biological',
            'cardboard': 'cardboard',
            'clothes': 'clothes',
            'glass': 'glass',
            'metal': 'metal',
            'paper': 'paper',
            'plastic': 'plastic',
            'shoes': 'clothes',
            'trash': 'trash'
        }

        return mapping

    def _build_dataset_configs(self):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        return {
            'trashnet': {
                'path': 'trashnet/dataset-resized',
                'prefix': 'trashnet',
                'file_patterns': ['*.jpg']
            },
            '12classes': {
                'path': '12classes/garbage_classification',
                'prefix': '12classes',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            },
            'WaRP': {
                'path': 'WaRP/merged_crops',
                'prefix': 'WaRP',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            },
            'garbage_classification_1': {
                'path': 'garbage_classification_1',
                'prefix': 'garbage_classification_1',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            },
            'trash_type': {
                'path': 'trash_type/TrashType_Image_Dataset',
                'prefix': 'trash_type',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            },
            'realwaste': {
                'path': 'realwaste',
                'prefix': 'realwaste',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            },
            'garbage_classification_2': {
                'path': 'garbage_classification_2',
                'prefix': 'garbage_classification_2',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            },
            'garbage_dataset': {
                'path': 'garbage_dataset/garbage-dataset',
                'prefix': 'garbage_dataset',
                'file_patterns': ['*.jpg', '*.jpeg', '*.png']
            }
        }

    def _calculate_image_metrics(self, image_path):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            with Image.open(image_path) as img:
                img_array = np.array(img)

                # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                width, height = img.size
                format_type = img.format if img.format else 'UNKNOWN'

                # –Ø—Ä–∫–æ—Å—Ç—å (—Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∏–∫—Å–µ–ª–µ–π –≤ grayscale)
                if len(img_array.shape) == 3:
                    gray = np.mean(img_array, axis=2)
                else:
                    gray = img_array
                brightness = np.mean(gray) / 255.0

                # –ö–æ–Ω—Ç—Ä–∞—Å—Ç (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ)
                contrast = np.std(gray) / 255.0

                # Edge score (–ø—Ä–æ—Å—Ç–æ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –≥—Ä–∞–Ω–∏—Ü)
                dy, dx = np.gradient(gray.astype(float))
                edge_score = np.mean(np.sqrt(dx**2 + dy**2)) / 255.0

                # Noise score (–≤–∞—Ä–∏–∞—Ü–∏—è –≤ –º–∞–ª–µ–Ω—å–∫–∏—Ö —É—á–∞—Å—Ç–∫–∞—Ö)
                noise_score = self._estimate_noise(gray)

                # –û–±—â–∏–π quality score
                quality_score = (
                    0.3 * (1 - abs(brightness - 0.5)) +  # –Ø—Ä–∫–æ—Å—Ç—å –±–ª–∏–∑–∫–∞ –∫ 0.5
                    0.3 * min(contrast * 3, 1.0) +       # –•–æ—Ä–æ—à–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
                    0.2 * min(edge_score * 5, 1.0) +     # –ß–µ—Ç–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
                    0.2 * (1 - min(noise_score * 10, 1.0))  # –ù–∏–∑–∫–∏–π —à—É–º
                )

                return {
                    'width': width,
                    'height': height,
                    'format': format_type,
                    'quality_score': round(quality_score, 4),
                    'brightness_score': round(brightness, 4),
                    'contrast_score': round(contrast, 4),
                    'edge_score': round(edge_score, 4),
                    'noise_score': round(noise_score, 4)
                }
        except Exception as e:
            print(f"Error processing {image_path}: {e}")
            return None

    def _estimate_noise(self, gray_image):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —à—É–º–∞ —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏—é –≤ –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–ª–æ–∫–∞—Ö
            h, w = gray_image.shape
            block_size = 8
            variances = []

            for i in range(0, h - block_size, block_size):
                for j in range(0, w - block_size, block_size):
                    block = gray_image[i:i+block_size, j:j+block_size]
                    variances.append(np.var(block))

            return np.mean(variances) / 255.0 if variances else 0.0
        except:
            return 0.0

    def _process_dataset(self, dataset_key):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª—é–±–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        config = self.dataset_configs.get(dataset_key)
        if not config:
            print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è {dataset_key} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return pd.DataFrame()

        dataset_path = self.raw_dir / config['path']
        records = []

        if not dataset_path.exists():
            print(f"‚ùå –ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
            return pd.DataFrame()

        print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {dataset_key} –∏–∑: {dataset_path}")

        total_images = 0
        classes_processed = 0

        for class_dir in dataset_path.iterdir():
            if class_dir.is_dir():
                original_class = class_dir.name
                
                # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
                if dataset_key in ['12classes', 'garbage_classification_1', 'garbage_classification_2', 'trash_type', 'garbage_dataset']:
                    original_class = original_class.lower()
                
                unified_class = self.class_mapping[config['prefix']].get(original_class)

                if not unified_class:
                    print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–ª–∞—Å—Å –≤ {dataset_key}: {original_class}")
                    continue

                print(f"  üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Å: {original_class} -> {unified_class}")

                image_count = 0
                for pattern in config['file_patterns']:
                    for img_path in class_dir.glob(pattern):
                        metrics = self._calculate_image_metrics(img_path)
                        if not metrics:
                            continue

                        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        image_id = f"{config['prefix']}_{original_class.replace(' ', '_')}_{img_path.stem}"
                        
                        records.append({
                            'image_id': image_id,
                            'file_path': str(img_path.relative_to(self.raw_dir)),
                            'dataset': config['prefix'],
                            'format': metrics['format'],
                            'unified_class': unified_class,
                            'width': metrics['width'],
                            'height': metrics['height'],
                            'split': '',
                            'quality_score': metrics['quality_score'],
                            'brightness_score': metrics['brightness_score'],
                            'contrast_score': metrics['contrast_score'],
                            'edge_score': metrics['edge_score'],
                            'noise_score': metrics['noise_score']
                        })
                        image_count += 1
                        total_images += 1

                print(f"    ‚úÖ {original_class}: {image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                classes_processed += 1

        print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ {dataset_key} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {classes_processed} –∫–ª–∞—Å—Å–∞—Ö")
        return pd.DataFrame(records)
    
    def assign_splits(self, df, train_size=0.6, val_size=0.2, test_size=0.2):
        """–ù–∞–∑–Ω–∞—á–∞–µ—Ç train/val/test splits —Å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º"""
        if len(df) == 0:
            return df

        train_val_idx, test_idx = train_test_split(
            df.index,
            test_size=test_size,
            stratify=df['unified_class'],
            random_state=42
        )

        train_idx, val_idx = train_test_split(
            train_val_idx,
            test_size=val_size/(1-test_size),
            stratify=df.loc[train_val_idx, 'unified_class'],
            random_state=42
        )

        df.loc[train_idx, 'split'] = 'train'
        df.loc[val_idx, 'split'] = 'val'
        df.loc[test_idx, 'split'] = 'test'

        return df

    def unify_datasets(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        print("Starting dataset unification...")

        all_data = pd.DataFrame()

        datasets_to_process = [
            'trashnet',
            '12classes', 
            'WaRP',
            'garbage_classification_1',
            'trash_type',
            'realwaste',
            'garbage_classification_2',
            'garbage_dataset'
        ]

        for dataset_key in datasets_to_process:
            print(f"\n{'='*50}")
            df = self._process_dataset(dataset_key)
            all_data = pd.concat([all_data, df], ignore_index=True)
            print(f"‚úÖ {dataset_key}: {len(df)} images")
            print(f"{'='*50}")

        print(f"\nTotal records collected: {len(all_data)}")

        if len(all_data) == 0:
            print("No records found! Check if datasets are downloaded correctly.")
            return 0

        print("Assigning train/val/test splits...")
        all_data = self.assign_splits(all_data)

        print(f"Saving manifest to {self.manifest_path}...")
        self.unified_dir.mkdir(parents=True, exist_ok=True)
        
        all_data.to_csv(self.manifest_path, index=False, encoding='utf-8')

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n=== Dataset Statistics ===")
        print(f"Total images: {len(all_data)}")
        print(f"By dataset:\n{all_data['dataset'].value_counts()}")
        print(f"By class:\n{all_data['unified_class'].value_counts()}")
        print(f"By split:\n{all_data['split'].value_counts()}")

        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print(f"\nImage quality stats:")
        print(f"Average quality score: {all_data['quality_score'].mean():.3f}")
        print(f"Average brightness: {all_data['brightness_score'].mean():.3f}")
        print(f"Average contrast: {all_data['contrast_score'].mean():.3f}")

        return len(all_data)


def main():
    unifier = DatasetUnifier()
    total_images = unifier.unify_datasets()
    print(f"\nUnification complete! Processed {total_images} images.")


if __name__ == "__main__":
    main()